---
title: "Theory of Finance PS1"
author: "Filip Mellgren, Oscar Krumlinde"
date: '2019-09-23'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include = FALSE}
library(rio)
library(tidyverse)
library(lubridate)
library(MASS)
source("P_perf.R")
source("Mean_var_frontier.R")
source("MVcalc.R")

```

# Import data
```{r}
comp <- import("data_ps1_final.xlsx", which = "components")
prices <- import("data_ps1_final.xlsx", which = "prices", skip = 1, setclass = "tibble")
Rfdf <- import("DTB3.csv")
```

# Data cleaning
```{r}
# Declare NAs.
Rfdf <- Rfdf %>% mutate(DTB3 = as.numeric(replace(DTB3, DTB3==".", NA)))
# Define the value that we need
rf <- mean(Rfdf$DTB3, na.rm = TRUE)/100
```

# Part I

```{r}
# Rename date column
prices <- prices %>% rename("date" = 1)

# Deselect companies
df <- prices %>% dplyr::select(-c("AMCR", "CTVA", "DOW", "FOX", "FOXA"))
```

## Q2

```{r}
# Calculate the arithmetic daily returns, store in a matrix, R
t <- nrow(df)
n <- ncol(df) -1#  # number of companies, minus 1 because of the date col 
# Calculate returns:
R <- as.matrix(df[2:t,2:(n+1)]-df[1:(t-1),2:(n+1)])/(df[1:t-1,2:(n+1)])

# Equal weights vector:
w <- as.matrix(rep(1/(n), n))

# Find the mean and variance and SR of the portfolio 
P_perf(R, w, 1, rf)

# Annualised
P_perf(R, w, 252, rf)
```

## Q3

```{r}
# Match the weights vector with the returns via the ticker symbol
# Transpose returns, add ticker, join weights based on ticker
P <- as.data.frame(cbind(t(R), names(R)))
P <- P %>% as_tibble() %>% rename("ticker" = V174)
P <- inner_join(P, comp, by = c("ticker" = "Symbol"))

# Check that it is sorted correctly:
all(names(R) == P %>% dplyr::select(ticker))

# Define the weights
w3 <- P$`Weight (%)` / 100

# Daily
P_perf(R, w3, 1, rf)

# Weekly
P_perf(R, w3, 5, rf)

# Monthly (assume the month has 21 days)
P_perf(R, w3, 21, rf)

# Annual
P_perf(R, w3, 252, rf)

```
## Q4

```{r}
# Annual return
r <- as.numeric(P_perf(R, w3, 252, rf)[2,1]) 
SD <- as.numeric(P_perf(R, w3, 252, rf)[2,2])
SR <- (r-rf)/SD

# Leverage ratio  
v <- c(1, 3, 5)

mu <- v*r + (1-v)*rf
sigma <- abs(v)*SD
sharpe <- (mu - rf)/sigma
```

## Q5

```{r}
r.daily <- as.numeric(P_perf(R, w3, 1, rf)[2,1])
A <- 1 # A for assets

# Last august occurs 2/3 into the year. 
days <- 2/3 * 252

return <- A * (1 + r.daily)^days

```

## Q6

For discussion


# Part II, mean variance frontiers

## Q2
```{r}
source("P_perf.R")
source("Mean_var_frontier.R")
source("MVcalc.R")

df2 <- df %>% dplyr::select(date, AAPL, MSFT)

# For the overview, we first provide a plot of indexed prices:
df2 %>% mutate(AAPL = AAPL/AAPL[1], MSFT = MSFT/MSFT[1]) %>% 
  gather(key = "stock", value = "Price", AAPL, MSFT) %>% 
  ggplot(aes(x = date, y = Price, color = stock)) + 
  geom_line() +
  labs(title = "Index AAPL vs MSFT")

# Actual correlation
df.MVF.a <- MVF(df2$AAPL, df2$MSFT, cor(df2$AAPL, df2$MSFT))
df.MVF.a %>% ggplot(aes(x = sigma, y = mustar, color = w)) + geom_point() +
  labs(title = "Actual correlation")

# correlation = -1
df.MVF.b <- MVF(df2$AAPL, df2$MSFT, -1)
df.MVF.b %>% ggplot(aes(x = sigma, y = mu, color = w)) + geom_point() +
  labs(title = "Correlation set to -1")

# correlation = 0
df.MVF.c <- MVF(df2$AAPL, df2$MSFT, 0)
df.MVF.c %>% ggplot(aes(x = sigma, y = mustar, color = w)) + geom_point() +
  labs(title = "Correlation set to 0")

# correlation = 1
df.MVF.d <- MVF(df2$AAPL, df2$MSFT, 1)
df.MVF.d %>% ggplot(aes(x = sigma, y = mustar, color = w)) + geom_point() +
  labs(title = "Correlation set to 1")

```

# Part III

## Q1
Find the weights of risky assets in the 30-stock tangency portfolio.

```{r}
# Data cleaning
# Filter observations to only include top 30 according to weight
stocks <- comp %>% filter(`Weight (%)` >= nth(comp$`Weight (%)`, 30))

# Create a new weight scheme such that it adds up to 1
stocks <- stocks %>% mutate(weight = `Weight (%)`/sum(`Weight (%)`))

# Keep the same companies in the prices data frame
prices_30 <- prices %>% dplyr::select(c(stocks$Symbol))
t <- nrow(prices_30)
n <- ncol(prices_30)

# Define the annualised mean returns and the covariance matrix
mu <- t(exp((prices_30[t, 1:n]/prices_30[1, 1:n]-1)*252/t) -1)
mu_e <- mu - rf
Sigma <- cov(prices_30)

```


```{r}
## Find the tangency portfolio weights
# Formula for calculating the tangency portfolio weights
ones <- rep(1, length(mu))
w_T <- (solve(Sigma)%*%mu_e) / as.numeric(t(ones) %*% solve(Sigma) %*% mu_e)

# Next, we compare the weights for the ten largest companies:
df3 <- data.frame(w_T, stocks$weight, stocks$Symbol) %>% top_n(10, w_T)

df3 %>% ggplot(aes(x = stocks.weight, y = w_T)) + geom_point() +
  labs(title = "No relation between S&P weight and the tangency portfolio weight", 
       x = "S&P weight", y = "Tangency portfolio weight") + 
  geom_smooth(method='lm',formula=y~x)

# Weights for reporting
df3 %>% dplyr::select(w_T, stocks.Symbol)
```
# Q2
```{r}
# Check if the largest 30 stocks of the S&P500 form the minimum-variance portfolio

# First, find weights for the 30 stocks MVP
w_mvp <- solve(Sigma) %*% ones / as.numeric(ones %*% solve(Sigma) %*% ones)

# Next, we compare the weights for the ten largest companies:
df32 <- data.frame(w_mvp, stocks$weight, stocks$Symbol)  %>% top_n(10, w_mvp)


df32 %>% ggplot(aes(x = stocks.weight, y = w_mvp)) + geom_point() +
  labs(title = "No relation between S&P weight and MVP weight", 
       x = "S&P weight", y = "MVP weight") + 
  geom_smooth(method='lm',formula=y~x)

df32 %>% dplyr::select(w_mvp, stocks.Symbol) %>% arrange(-w_mvp)


```

# Q3
```{r}
# Variance MVP, estimated using historic data
MVP <-  as.matrix(prices_30) %*% w_mvp
as.numeric(var(MVP))

# Variance TP, estimated using historic data
TP <-  as.matrix(prices_30) %*% w_T
as.numeric(var(TP))

# Covariance MVP and the tangency portfolio
as.numeric(cov(MVP, TP))

cor(MVP, TP)




```
## Q4
Is the whole S&P500 the tangency portfolio? Calculate the weights of all 500 constitutes inside the tangency portfolio. Comment on the difference between the top10 constitutes between this tangency portfolio and 30-stock tangency portfolio from (1)
 
```{r}
stocks_bonus <- comp %>% filter(!Symbol %in% c("AMCR", "CTVA", "DOW", "FOX", "FOXA"))
prices_bonus <- prices %>% 
  dplyr::select(-c("AMCR", "CTVA", "DOW", "FOX", "FOXA")) %>%
  dplyr::select(-"date")

t <- nrow(prices_bonus)
n <- ncol(prices_bonus)

# Define the annualised mean returns and the covariance matrix
mu <- t(exp((prices_bonus[t, 1:n]/prices_bonus[1, 1:n]-1)*252/t) -1)
mu_e <- mu - rf
Sigma <- cov(prices_bonus)

```


```{r}
## Find the tangency portfolio weights
# Formula for calculating the tangency portfolio weights
# Since the covariance matrix is not invertible...
# ...we use a generalised inverse procedure (Moore-Penrose)
# The colinearity/cointegration arise because of them all being dependent...
# ...on common systemic risk and because the time scale is short meaning...
#... that linear dependence might arise by chance.

ones <- rep(1, length(mu))
w_T <- (ginv(Sigma)%*%mu_e) / as.numeric(t(ones) %*% ginv(Sigma) %*% mu_e)

df_bonus <- data.frame(w_T, stocks_bonus$`Weight (%)`, stocks_bonus$Symbol)
df_bonus %>% filter(stocks_bonus.Symbol %in% c("AAPL", "AMZN", "PEP", "WMT", "SBUX", "CAT", "MS", "GPS", "EXPE"))

# Comment on the top 10 constitutes between this portfolio and the tangency portfolio

df_bonus %>% top_n(10, w_T) %>% arrange(-w_T) %>% dplyr::select(w_T, stocks_bonus.Symbol)
```
First note that both portfolios are efficient, there is no idiosyncratic risk that can be diversified away by combining two portfolios. Hence, the two portfolios contain systemic risk up to a varying degree which is measured by the correlation between the two. Therefore $\sigma_{mvp} = \rho_{mvp, tp} \sigma_{tp} (*) $. No constant needs to be added to make the equality hold because that would mean there were idiosyncratic risk left in one of the portfolios, contradicting the definition of the efficiency frontier. As a final step, we note that the relationship (*) is equivalent to what was asked for which is obtained by multiplying with both sides with $\sigma_{mvp}.$

The remaining common risk is measured by the correlation between the two. 


